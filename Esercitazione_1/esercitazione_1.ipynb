{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-03T14:57:57.230841Z",
     "start_time": "2025-09-03T14:57:55.790837Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from numpy.linalg import norm\n",
    "from prettytable import PrettyTable\n",
    "import textwrap\n",
    "from sys import exit\n",
    "\n",
    "\n",
    "from torchgen.native_function_generation import return_str\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**UTILS**",
   "id": "d87bfaddb2676e94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T14:57:58.444462Z",
     "start_time": "2025-09-03T14:57:58.441983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "def get_synset_or_first(word_or_synset: str):\n",
    "    try:\n",
    "        # Try to interpret the input as a synset name (\"dog.n.01\")\n",
    "        return wn.synset(word_or_synset)\n",
    "    except:\n",
    "        # If it fails, interpret it as a word\n",
    "        synsets = wn.synsets(word_or_synset)\n",
    "        if synsets:\n",
    "            return synsets[0]  # return the first synset in the list (usually the most common sense)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def wrap_text(text, width=20):\n",
    "    return \"\\n\".join(textwrap.wrap(text, width=width))"
   ],
   "id": "6294cfdd07b22f57",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**EMBEDDING SPACE BUILDING**",
   "id": "209d339093c10944"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T14:58:13.955781Z",
     "start_time": "2025-09-03T14:57:58.453492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_synsets = list(wn.all_synsets())\n",
    "\n",
    "# load the definition of each synset in wordnet\n",
    "synsets_tokens = []\n",
    "for synset in wn.all_synsets():\n",
    "    synset_signature = synset.definition()\n",
    "    synsets_tokens.append(synset_signature)\n",
    "\n",
    "# map the definition in an Embedding space\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"thenlper/gte-small\", cache_folder=\"model\", local_files_only = True)\n",
    "synsets_embeddings = model.encode(synsets_tokens, show_progress_bar=True, random_state=42, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "\n",
    "save_path = \"embeddings.npz\"\n",
    "\n",
    "print(f\"Saving embeddings in {save_path} ...\")\n",
    "np.savez_compressed(save_path, embeddings=synsets_embeddings, synsets=[s.name() for s in all_synsets])\n",
    "print(\"Fatto.\")\n",
    "\n"
   ],
   "id": "34147ff04ecf0f9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrea/Desktop/UNIVERSITÀ/CORSI/Tecnologie del Linguaggio Naturale /NLP [3]/Esercitazioni/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches:   6%|▌         | 217/3677 [00:09<02:27, 23.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[32m     11\u001B[39m model = SentenceTransformer(\u001B[33m\"\u001B[39m\u001B[33mthenlper/gte-small\u001B[39m\u001B[33m\"\u001B[39m, cache_folder=\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, local_files_only = \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m synsets_embeddings = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43msynsets_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_to_numpy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# save the embeddings in a local file\u001B[39;00m\n\u001B[32m     16\u001B[39m save_path = \u001B[33m\"\u001B[39m\u001B[33membeddings.npz\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/UNIVERSITÀ/CORSI/Tecnologie del Linguaggio Naturale /NLP [3]/Esercitazioni/venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/UNIVERSITÀ/CORSI/Tecnologie del Linguaggio Naturale /NLP [3]/Esercitazioni/venv/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1087\u001B[39m, in \u001B[36mSentenceTransformer.encode\u001B[39m\u001B[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001B[39m\n\u001B[32m   1085\u001B[39m             \u001B[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001B[39;00m\n\u001B[32m   1086\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m convert_to_numpy:\n\u001B[32m-> \u001B[39m\u001B[32m1087\u001B[39m                 embeddings = \u001B[43membeddings\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1089\u001B[39m         all_embeddings.extend(embeddings)\n\u001B[32m   1091\u001B[39m all_embeddings = [all_embeddings[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m np.argsort(length_sorted_idx)]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**EMBEDDING SPACE USAGE**",
   "id": "cac04a488ea4cefe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T14:58:14.034975Z",
     "start_time": "2025-08-30T14:21:49.940927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    # load the embedding space from the file\n",
    "\n",
    "    data = np.load(embeddings_path, allow_pickle=True)\n",
    "    embeddings = data['embeddings']\n",
    "    synset_names = data['synsets']\n",
    "\n",
    "    # create a dict [Synset: Definition]\n",
    "    synsets_embeddings = dict(zip(synset_names, embeddings))\n",
    "\n",
    "    return synsets_embeddings\n",
    "\n",
    "def most_similar_embedings(synsets_embeddings, target_synset, pos=None, k=5):\n",
    "\n",
    "    # get the embedding associated to the target synset\n",
    "    target_vec = synsets_embeddings[target_synset.name()]\n",
    "\n",
    "    similarities = []\n",
    "    # compute the cosine imilarity between the target synset embedding and all the others in the vector space\n",
    "    for syn in wn.all_synsets():\n",
    "        if syn.name() == target_synset.name():\n",
    "            continue\n",
    "        sim = cosine_similarity(target_vec, synsets_embeddings[syn.name()])\n",
    "        similarities.append((syn, sim))\n",
    "\n",
    "    if pos:\n",
    "        similarities = [elem for elem in similarities if elem[0].pos() == pos]\n",
    "    # ordina per similarità decrescente\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:k]\n"
   ],
   "id": "47227042696b82e6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T14:58:14.044116Z",
     "start_time": "2025-08-30T14:21:52.291165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings_path = \"embeddings.npz\"\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "# TARGET WORD\n",
    "target = \"computer\"\n",
    "\n",
    "# get the WordNet Synset associated at the target word\n",
    "target_synset = get_synset_or_first(target)\n",
    "\n",
    "if target_synset is None:\n",
    "    print(f\"synset for {target} not found.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# get the most (semantic) symilar synset\n",
    "similar_synsets = most_similar_embedings(embeddings, target_synset, pos=target_synset.pos(), k=5)\n",
    "\n",
    "\n",
    "\n",
    "# RESULT OUTPUT\n",
    "print(f\"--- WORDNET RESULT FOR {target_synset.name()} ---\")\n",
    "\n",
    "wordnet_table = PrettyTable(hrules=True, header=False)\n",
    "width = 30\n",
    "wordnet_table.add_row([\"GLOSS (DEFINITION)\", wrap_text(str(target_synset.definition()), width)])\n",
    "if target_synset.examples():\n",
    "    wordnet_table.add_row([\"USAGE EXAMPLE\", wrap_text(str(target_synset.examples()[0]), width)])\n",
    "\n",
    "wordnet_table.add_row([\"LEMMAS\", wrap_text(str(target_synset.lemma_names()), width)])\n",
    "\n",
    "wordnet_table.add_row([\"HYPERNYMS\", wrap_text(str(target_synset.hypernyms()[:5]) + \"...\", width)])\n",
    "wordnet_table.add_row([\"HYPONYMS\", wrap_text(str(target_synset.hyponyms()[:5]) + \"...\", width)])\n",
    "\n",
    "print(wordnet_table)\n",
    "\n",
    "embeddings_table = PrettyTable(hrules=True, field_names = [\"SYNSET\", \"DESCRIPTION\", \"SEMSIM\"])\n",
    "\n",
    "for synset in similar_synsets:\n",
    "    synset_name = synset[0].name()\n",
    "    synset_definition = synset[0].definition()\n",
    "    synset_similarity = round(float(synset[1]), 2)\n",
    "\n",
    "    embeddings_table.add_row([synset_name, wrap_text(str(synset_definition), width), synset_similarity])\n",
    "\n",
    "print(f\"--- EMBEDDINGS RESULT FOR {target_synset.name()} ---\")\n",
    "print(embeddings_table)\n",
    "\n"
   ],
   "id": "36d76dde0313e56f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WORDNET RESULT FOR computer.n.01 ---\n",
      "+--------------------+--------------------------------+\n",
      "| GLOSS (DEFINITION) |    a machine for performing    |\n",
      "|                    |   calculations automatically   |\n",
      "+--------------------+--------------------------------+\n",
      "|       LEMMAS       |          ['computer',          |\n",
      "|                    |      'computing_machine',      |\n",
      "|                    |      'computing_device',       |\n",
      "|                    |       'data_processor',        |\n",
      "|                    | 'electronic_computer', 'inform |\n",
      "|                    |   ation_processing_system']    |\n",
      "+--------------------+--------------------------------+\n",
      "|     HYPERNYMS      |  [Synset('machine.n.01')]...   |\n",
      "+--------------------+--------------------------------+\n",
      "|      HYPONYMS      | [Synset('home_computer.n.01'), |\n",
      "|                    |         Synset('pari-          |\n",
      "|                    |     mutuel_machine.n.01'),     |\n",
      "|                    | Synset('turing_machine.n.01'), |\n",
      "|                    | Synset('node.n.08'), Synset('d |\n",
      "|                    |   igital_computer.n.01')]...   |\n",
      "+--------------------+--------------------------------+\n",
      "--- EMBEDDINGS RESULT FOR computer.n.01 ---\n",
      "+----------------------+-------------------------------+--------+\n",
      "|        SYNSET        |          DESCRIPTION          | SEMSIM |\n",
      "+----------------------+-------------------------------+--------+\n",
      "|   calculator.n.02    |  a small machine that is used |  0.95  |\n",
      "|                      | for mathematical calculations |        |\n",
      "+----------------------+-------------------------------+--------+\n",
      "|      adder.n.02      |  a machine that adds numbers  |  0.92  |\n",
      "+----------------------+-------------------------------+--------+\n",
      "|   calculator.n.01    |  an expert at calculation (or |  0.92  |\n",
      "|                      |    at operating calculating   |        |\n",
      "|                      |           machines)           |        |\n",
      "+----------------------+-------------------------------+--------+\n",
      "|   subtracter.n.02    |    a machine that subtracts   |  0.92  |\n",
      "|                      |            numbers            |        |\n",
      "+----------------------+-------------------------------+--------+\n",
      "| number_cruncher.n.02 |     a computer capable of     |  0.91  |\n",
      "|                      |  performing a large number of |        |\n",
      "|                      |  mathematical operations per  |        |\n",
      "|                      |             second            |        |\n",
      "+----------------------+-------------------------------+--------+\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3cd67a7b3a684645"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
